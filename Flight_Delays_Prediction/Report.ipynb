{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dbe14ea-01de-43cb-a11e-49705929fff4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Estimated Time of Delay ✈ \n",
    "## ML Based Flight Departure Delay Prediction \n",
    "### Group Project (4 members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f8d023f-351d-46af-81c7-5c97242c055b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Project Abstract\n",
    "\n",
    "According to the Bureau of Transportation Statistics, in August 2022, approximately 25% of flights to US airports arrived 15 minutes or more late<sup>1</sup>. These delays generate financial losses and impact customer experience. The ability to predict flight delays will allow the operations teams to take earlier action to accommodate customers, which is the goal of this project.\n",
    "\n",
    "The source data used is the instructor-provided post-join data set with ~42 million rows. A rolling time series split cross validation was used during training. Linear regression, Decision tree, Random Forest and Gradient-Boosted Tree regression models were developed to predict departure delays. Gradient-Boosted Tree produced the best result in terms of prediction accuracy, but the Decision Tree model produces performance close to it but at a significantly more reasonable time. A performance/training time tradeoff will be discussed. A classfication task was also performed whether or not the flight would experienced a delay. Multiperceptron neural network with one hidden layer gave the best result.\n",
    "\n",
    "### 1.2 Business Case\n",
    "\n",
    "The prediction task can be viewed as a regression problem where the goal is to predict the actual delay time of a flight in minutes, or a classification task in which the model predicts whether there will be delay or not. A flight is regarded as being delayed if it is delayed 15mins or greater. The ability of the airline operations teams to predict if a flight will be delayed and how long the delay will be provides opportunities to take earlier actions to accomodate customer needs. By proactively offering alternative flight recommendations, discounts, or lounge vouchers after a flight delay prediction, customer experience will be improved, which can maintain customer demand and mitigate financial losses for the airline.\n",
    "\n",
    "### 1.3 Research Question / Hypothesis\n",
    "\n",
    "This machine learning project experimented on various prediction models to solve the delayed flights business case scenario. The main question we will be answering is:\n",
    "\n",
    "*What customized features of machine learning pipelines best accurately predict delayed flights?*\n",
    "\n",
    "Our hypothesis is that adding engineered features to our pipelines will increase a model's performance. Undersampling, oversampling and SMOTE strategies to correct an imbalanced binary class distribution will also affect model performances. Oversampling increases likelihood of a model to overfit and may impact performance. We will experiment on various regression models such as Linear Regression, Decision Tree, Random Forest, Gradient Boosted Tree, as well as classification models such as Decision Tree, Random Forest, Gradient Boosted Tree and Multilayer Perceptron Neuron Network. Various network architecture will be explored.  Experimental findings will be discussed in later sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e43c3d5-81d7-4003-9385-8e4dba9eafb1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Data and Feature Engineering\n",
    "\n",
    "\n",
    "### 2.1 Data Summary\n",
    "\n",
    "We chose to use the instructor-provided data set, which already has joined the individual stations, and flights data with the weather data which has 42,271,633 rows and 168 columns. A summary of the count of flights in each class of delay length is shown below and shows that the vast majority of flights are on time or early, and those that are delayed are generally not delayed by more than 15-30 minutes. \n",
    "\n",
    "<img src=\"https://github.com/stackoverthrow/261section7group2/blob/main/class_distribution.jpeg?raw=true\" width=80%>\n",
    "\n",
    "### 2.2 Data Lineage\n",
    "\n",
    "The working data is comprised from three separature individual data frames below. During the initial phases of the project we produced our own joined data set following the strategy shown in the diagram below. To enable more consistent results with our peers in the leader boards and to enable more focus on the model tuning we decided to use the instructor joined data set. Based on the names of the columns in the data set and number of rows it seems the join strategy is very similar to the one we employed in our phase 3 notebook. After loading the instructor data set we separately engineered several sub-dataframes representing the engineered features. For example the volume_sigma is calculated on an airport and day grouping and then the resultant volume_sigma value is joined to the main data set for all flights with the same date and airport. There were a few missing results in the engineered features resulting in loss of 27 rows when joining back to the main data set. This loss of data is insignificant relative to the 42M+ rows that remain. \n",
    "* Flights data from [BTS](https://www.transtats.bts.gov/DatabaseInfo.asp?QO_VQ=EFD&DB_URL=)\n",
    "* Weather data from [NCEI](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00679)\n",
    "* Weather station data [DOT](dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data)\n",
    "\n",
    "<img src=\"https://github.com/stackoverthrow/261section7group2/blob/main/data_lineage_table.jpeg?raw=true\" width=70%>\n",
    "\n",
    "### 2.3 List of Family Features\n",
    "\n",
    "The list of family features and their EDA are summarized in this notebook:\n",
    "  * [List of Family Features and EDA](https://github.com/soggiugiorgio/Portfolio_Projects/blob/main/Flight_Delays_Prediction/Feature_Families_EDA.ipynb).\n",
    "  \n",
    "The family of features explored from the full data set prior cleaning are as follows: \n",
    "- **Arriving/Departing Flight** - features describing the flight characteristics such as time of arrival and departure dates and times, traffic conditions, delay and cancellation statuses\n",
    "- **Airport of Origin/Destination** - features describing airport characteristics such as the ID, name, type, and location of origin and destination of each flight records\n",
    "- **Airplane/Airline** - features identifying airline and airplane\n",
    "- **Weather** - features describing physical characteristics of weather conditions during the flight\n",
    "- **Engineered Flight/Airport features** - features not in original data set but found to have stonger correlation with the target variable for better modeling. These are further discussed in sections 3.4 and 3.5.\n",
    "\n",
    "<img src ='https://github.com/noriiii/W261_Final_Project_NB/blob/main/FeaturesFamily.png?raw=true'>\n",
    "\n",
    "\n",
    "### 2.4 Feature Engineering\n",
    "\n",
    "In addition to the base features in the provided data sets we created several features based on our research and intuition about other factors related to potential delays. The details of each engineered features, including the resultant distribution of each engineered feature, is included in the [feature engineering](https://github.com/soggiugiorgio/Portfolio_Projects/blob/main/Flight_Delays_Prediction/Feature%20engineering.ipynb) notebook. \n",
    "* *Delays in Last 4 hours*: Partitioned by each origin airport, count the number of flights that have been delayed >=15 minutes in the past 4 hours as that airport. \n",
    "* *Inbound Departure Delay*: Was the last flight with the same tail-number as the plane schdeduled for this departure delayed? \n",
    "* *Departure Volume*: How many standard deviations away from the average # of scheduled departures for each airport is the count of schedules depatures for today? \n",
    "* *Page-Rank*: How important is the airport to overall flight connectivity? \n",
    "* *Holidays*: How many days away is the next major US holiday from the scheduled flight date?\n",
    "\n",
    "<img src=\"https://github.com/stackoverthrow/261section7group2/blob/main/feature_eng.jpeg?raw=true\" width=70%>\n",
    "\n",
    "The delays in last 4-hours and inbound departure delay features were achieved with a SQL windowing function as shown in the code block below. The flight data is first partitioned by origin airport, sorted by pre-departure time and then the given calculation (in this case sum of delays) is performed over a window defined by the 'BETWEEN' keyword as shown below. This general framework is very useful for time series feature engineering and was employed several times in the feature engineering process. \n",
    "\n",
    "<img src=\"https://github.com/stackoverthrow/261section7group2/blob/main/window_func.jpeg?raw=true\" width=70%>\n",
    "\n",
    "### 2.5 Feature Correlation\n",
    "\n",
    "<img src=\"https://github.com/noriiii/W261_Final_Project_NB/blob/main/correlation%20.png?raw=true\" width=80%>\n",
    "\n",
    "The raw weather parameters did not have a very strong correlation with departure delay as shown in the correlation matrix and sub-set barplot for departure delay time shown below. The most strongly correlated features were two of the engineered features we created in Phase 3, which are 'inbound_dep_delay_time' and 'delays_last_4_hrs'. This result is strong motivation to continue being more creative with additional feature engineering and manipulation into phase 4 to further enhance the predictive power beyond the available features, so we added two new engineered features, \"page_rank\" and \"is_holiday.\"\n",
    "\n",
    "**Top 5 Features:**\n",
    "\n",
    "<img src=\"https://github.com/noriiii/W261_Final_Project_NB/blob/main/topfeaturesnew.png?raw=true\" width=40%>\n",
    "\n",
    "### 2.6 Handling Imbalanced Dataset\n",
    "\n",
    "In the case of the classification models, the 5-to-1 imbalance of the on-time vs delayed flight counts will result in challenges training model unsuccessfully. For phase 4, we chose to handle imbalanced dataset by majority down-sampling, minority oversampling and SMOTE oversampling and compared which of this three balancing method yields the best prediction modeling results. As summarized in the below table, the on-time vs delayed flight counts was successfuly balanced to a 1-to-1 ratio.\n",
    "\n",
    "<img src=\"https://github.com/noriiii/W261_Final_Project_NB/blob/main/ProportionTarget.png?raw=true\">\n",
    "\n",
    "**Majority Down-sampling**\n",
    "\n",
    "The majority class of non-delayed flights was downsampled to more closely match the minority delayed flight count as shown in the chart below. \n",
    "\n",
    "<img src=\"https://github.com/noriiii/W261_Final_Project_NB/blob/main/Undersampling.PNG?raw=true\">\n",
    "\n",
    "**Minority Over-sampling**\n",
    "\n",
    "The minority class of delayed flights was oversampled to more closely match the majority delayed flight count as shown in the chart below.\n",
    "\n",
    "<img src=\"https://github.com/noriiii/W261_Final_Project_NB/blob/main/Oversampling.PNG?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "846a2f1a-57bc-4d4a-bcb3-1b83c8ab5c9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##3. Data Leakage\n",
    "\n",
    "### 3.1 Definition of Leakage & Potential examples\n",
    "\n",
    "**Data leakage** occurs when information outside training data set is used unintentionally to train a model, or information from the validation data set is used in the training process. This concern is particularly important in the case of time-series data where future information or data not available at the time of the prediction should not be used as a part of the model building process. \n",
    "\n",
    "While we did take care to implement a series based approach to splitting our training and validation sets as described in section 4.2 below, there are few details of the feature engineering process that could be considered 'sinful'. The window based functions are inherently okay due to their focus on only the window of data preceding the given row. The approach we took to definining the business of the airport through the 'volume_sigma' and 'page_rank' features, however, were a little bit naughty. We used the full data set to establish the baseline distribution of average # of flights for each airport, and the baseline standard deviation. Similarly the 'page_rank' values for each airport were arrived through anaylsis of the entire data set. One can argue that these distribution based features are subject to the law of large numbers after a year or so of data given the lack of changes in the distribution from year to year, and that the leakge is unlikely to change the overall distribution estimate significantly. That being said, a better approach would have been to extend the data set into 2014 or before and use that to establish the 'historical' distribution rather than using the full set. Neither the page-rank nor the volume_sigma feature ended up being very important compared to the window based features and were not actually included in the final best performing model, thus absolving us of our ML sins. \n",
    "\n",
    "### 3.2 Train-Test Split\n",
    "\n",
    "The nature of the flight delay prediction task entails a limitation about the information known at the time of each prediction. A standard randomized split of the data rows into train and test sets would result in 'leakage' of future information into the training set for past predictions. At the same time the flight delay probability varies significantly over the course of the seasons of the year so it is critical that we maintain at least a single continuous year of data in each training fold during the cross-validation process. To accomplish this task we will simply sort the training data by time-stamp and then use a rolling time series split like shown in the first table below, in which we train on a window that increases by 1 year with each fold, while the validation year also indexes by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48413931-26d9-4512-bf0e-33f75f1a2804",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##4. Modeling Pipelines\n",
    "\n",
    "### 4.1 Modeling Pipeline Summary\n",
    "\n",
    "A simplified, high-level overview of the pipeline our team constructed is shown in the figure below. The execution of each key step a code level is included [here](https://github.com/soggiugiorgio/Portfolio_Projects/blob/main/Flight_Delays_Prediction/Regression_engineered_features.ipynb). The following also provide quick-links to the separated notebooks for each analysis. We smoothly integrate the pipeline more for data preparation and feature generation through dbutils.notebook.run commands in phase 4. \n",
    "  * [Regression with raw features](https://github.com/soggiugiorgio/Portfolio_Projects/blob/main/Flight_Delays_Prediction/Regression_no_engineered_features.ipynb).\n",
    "  * [Regression with engineered features](https://github.com/soggiugiorgio/Portfolio_Projects/blob/main/Flight_Delays_Prediction/Regression_engineered_features.ipynb).\n",
    "  * [Classification with engineered features](https://github.com/soggiugiorgio/Portfolio_Projects/blob/main/Flight_Delays_Prediction/Classification_pipeline_new_features.ipynb)\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/stackoverthrow/261section7group2/blob/main/3_pipeline.jpeg?raw=true\" width=90%>\n",
    "\n",
    "**Data Load & Pre-treatment**\n",
    "* Remove sparse columns \n",
    "* Drop low % null sparse rows \n",
    "* Impute missing rows \n",
    "\n",
    "\n",
    "### 4.2 Families of input features\n",
    "<img src=\"https://github.com/noriiii/W261_Final_Project_NB/blob/main/SelectedFeaturesFamily.png?raw=true\">\n",
    "\n",
    "There are nineteen (19) input features used in modeling and they belong to the following feature families:\n",
    "- **Engineered Flight/Airport** (3 features)\n",
    "- **Arriving/Departing Flight** (1 feature) \n",
    "- **Weather** (15 features)\n",
    "\n",
    "None of the features from Airport of Origin/Destination and Airplane/Airline families were used in modeling, as none of them were found to have strong correlation with the target variable.\n",
    "\n",
    "The list of family features and their EDA are summarized in this notebook:\n",
    "  * [List of Family Features and EDA](https://github.com/soggiugiorgio/Portfolio_Projects/blob/main/Flight_Delays_Prediction/Feature_Families_EDA.ipynb).\n",
    "\n",
    "The settings considered for conducting experiments in our modeling pipeline are working under undersampled, oversampled and SMOTE oversampled data, as well as including and excluding engineered features. All were successfully implemented except SMOTE oversampling due to the difficulties when implementing this method. Hyperparameter discussion were discussed in the section 6 below.\n",
    "\n",
    "### 4.3 Loss Function\n",
    "\n",
    "**Regression**\n",
    "\n",
    "For the sake of readability we have separated the code for data loading, processing, feature engineering and regression model development in separatre notebooks:\n",
    "* [Regression with raw features](https://github.com/soggiugiorgio/Portfolio_Projects/blob/main/Flight_Delays_Prediction/Regression_no_engineered_features.ipynb).\n",
    "* [Regression with engineered features](https://github.com/soggiugiorgio/Portfolio_Projects/blob/main/Flight_Delays_Prediction/Regression_engineered_features.ipynb).\n",
    "\n",
    "\n",
    "The intiial primary goal of this project was to predict the actual flight departure delay time in minutes represented by the DEP_DELAY_NEW variable with only data available greater than two hours from the scheduled departure time. In the case of regression the loss function for gradient descent during model training was the Mean Square Error or MSE defined as: \n",
    "\n",
    "$$MSE = \\frac{\\sum_{i=1}^{n}{(y_i  -  \\hat{y})^2}}{N} $$\n",
    "\n",
    "We may also be interested in the coefficient of variation but not for the purposes of model training. The coefficient of variation, or R2 is defined as:\n",
    "\n",
    "$$R^2 = 1 - \\frac{Unexplained Variation}{Total Variation} = 1 - \\frac{\\sum_{i=1}^{n}{(y_i  -  \\hat{y})^2}}{\\sum_{i=1}^{n}{(y_i - \\bar{y})^2}} $$\n",
    "\n",
    "**Classification**\n",
    "\n",
    "For the sake of readability we have deparated the code for classification model development [here](https://github.com/soggiugiorgio/Portfolio_Projects/blob/main/Flight_Delays_Prediction/Classification_pipeline_new_features.ipynb).\n",
    "\n",
    "During phase 1 and 2 we discovered the task of accurately predicting the delay time in minutes may be much more challenging than expected and suspected that a simple binary classification of whether a flight will or will not be delayed may be more successful. When switching to a classification task we will adjust our loss function from MSE to the categorical cross-entropy which is defined by the following relationship: \n",
    "\n",
    "$$Cross Entropy = -\\frac{1}{M}\\sum_{i=1}^{M}{y_i\\log{\\hat{y_i}} + (1-y_i)log(1 - \\hat{y_i})}$$\n",
    "\n",
    "In addition to the cross-entropy loss metric which will be used for the actual model training process we will also evaluate the effectiveness of our models using the area under precision-recall curve as well as the base, precision, recall, and F1 scores to evaluate the predictive power of our classifcation models. \n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "$$Recall = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "$$F1_{score} = \\frac{2(Recall)(Precision)}{Recall + Precision}$$\n",
    "\n",
    "### 4.4 Experiment Summary\n",
    "\n",
    "<img src =\"https://github.com/noriiii/W261_Final_Project_NB/blob/main/ExperimentSummary.png?raw=true\">\n",
    "\n",
    "As shown in the Experiment Summary table, there are a total of eighteen (18) experiments conducted between various regression and classification models using either the full dataset, downsampled data or oversampled data. Additionally, eight (8) experiments conducted for Multilayer Perceptron Neural Network on different network architectures for undersampled data, which are discussed further in the next section. The training and evaluation times for each experiement are also reported in the table. \n",
    "\n",
    "The experiments include regression models such as Linear, Decision Tree, Random Forest and Gradient Boosted Tree. All regression models were experimented with or without engineered features included. \n",
    "\n",
    "The models experimented for binary classification include Decision Tree, Random Forest, Gradient Boosted, Logistic Regression and Multilayer Perceptron Neural Network. All classification models were experimented under downsampling and oversampling conditions.\n",
    "\n",
    "The experimental results are discussed in Section 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63e75620-4e61-4e10-8613-caa1f77d60cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##5. Neural Network\n",
    "\n",
    "\n",
    "As part of this project, we have experimented on different neural network architectures and also experimented with training at different epochs and using different optimizers. The image below shows the experimental set up and results.\n",
    "\n",
    "\n",
    "* [Multiperceptron Experimental Databricks Notebook](https://github.com/soggiugiorgio/Portfolio_Projects/blob/main/Flight_Delays_Prediction/Classification_MLP_Expt.ipynb).\n",
    "\n",
    "\n",
    "<img src =\"https://github.com/noriiii/W261_Final_Project_NB/blob/main/NeuralNetwork.png?raw=true\">\n",
    "\n",
    "As can be seen above, the first three models shows that as we decrease the number of hidden layers from 3 to 1, the validation area under the ROC curve increases with a significant increase in value when we moved from three hidden layers to two hidden layers and of course out of these three models, the model with one hidden layer (19-38-Sigmoid-2-Softmax) gave the best results. This is because for this data, the model probably started to overfit the training data. Hence, only one hidden layer should be used with this data.\n",
    "\n",
    "Since, we got a better result using one hidden layer (19-38-Sigmoid-2-Softmax) and using gradient descent as the optimizer, we decided to explore another optimizer called l-bfgs which is provided as part of the pyspark package to compare such output with the model created with gradient descent keeping all other factors constant.\n",
    "\n",
    "l-bfgs is otherwise called limited-memory BFGS where BFGS is named after its creators: Broyden, Fletcher, Goldfarb, and Shanno. l-bfgs is a second order optimization algorithm (unlike gradient descent which is a first order optimization algorithm) found in the family of the quasi newton algorithms and simply used to help minimize the cost function.\n",
    "\n",
    "Our result showed that keeping all other factors constant, using l-bfgs (19-38-Sigmoid-2-Softmax(l-bfgs, maxIter=100)) gave a better performance than using gradient descent (19-38-Sigmoid-2-Softmax) as the optimizer. Due to this result, we then decided to perform another set of experiments using one hidden layer, l-bfgs as the optimizer, keeping all other factors constant but changing the maxIter(epoch) value each time (50, 100, 150, 200, 300). Our result showed that an increase in the epochs (maxIter) translates to an increase in the model performance. However, as we move from epoch size of 250 to 300 (see image below), we started to see a slow upward trend which signifies that if we continue to increase the maxIter value, a point would likley set in where there would be no more increase in model performance. According to our experiments, **(19-38-Sigmoid-2-Softmax(l-bfgs, maxIter=300))** gave the best model performance on the test sets.\n",
    "\n",
    "<img src =\"https://github.com/YemiOlani/w261_images/blob/main/MLP_result_2.JPG?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34b17854-24ee-4aa5-afdd-2af17b90bb7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6. Results & Discussion\n",
    "\n",
    "As shown on section 5.4, within the regression prediction task, the decision tree model with engineered features has the best overall performance in terms of relative balance of loss and training time. While the gradient-boosted tree model has the lowest loss, its loss is not significantly different from the decision tree model and it has a very significant training time. The random forest also performed slightly better but with a significant training time burden that does not appear to be worth the trouble. Hence, we conclude that our final prediction pipeline is the decision tree regression in phases 2, 3 and 4. The inclusion of the engineered features did slightly improve the loss compared to their exclusion.\n",
    "\n",
    "However, none of the regression models performed better than 10% over the naive baseline. So on phase 4, we decided to focus on experimenting on binary classification model to find the best performance. In the classification task, the logistic regression performed the best in terms of AUC. While Random Forest and Multilayer Perceptron Neural Network (19-38-Sigmoid-2-Softmax(l-bfgs, maxIter=300)) models produced the highest AUC, the logistic regression has significantly lower amount of time to produce nearly the same AUC value that Random Forest, Gradient-Boosted and Multilayer Perceptron Neural Network models produce. Hence, our final classification pipeline for phase 4 is the logistic regression, which is a confirmation of what we also found in phase 3. The classification models typically performed better on oversampled data, but Logistic Regression trained and evaluated faster on undersampled data.\n",
    "\n",
    "While we have sufficient amount of successful experiments to make our final conclusion, we had challenges encountered in all phases. For example, cancelling XgBoost due to programming error on Phase 2, Gradient-Boosted taking longer than 3 days for regression task on Phase 3, and having difficulty implementing SMOTE oversampling on Phase 4 due to programming errors and time-consuming runs. These challenges are motivations for us to continue to improve in our future work in experimenting and finding the best performing regression and classification models.\n",
    "\n",
    "**Gap Analysis**\n",
    "\n",
    "A brief summary of our analysis of our gaps from the leading groups is included in the figure below. \n",
    " \n",
    "<img src=\"https://github.com/noriiii/W261_Final_Project_NB/blob/main/GapAnalysis.png?raw=true\" width=60%>\n",
    "\n",
    "In comparison to other teams, our logistic regression produced the third highest AUC for both  cross fold validation and blind test set performances. Team “Section 5 Group 1” reported the highest AUC of >0.90, but is unclear which of their models yielded this result. Our logistic regression trained faster than most teams; however, it is not the fastest. “FP_Section4_Group4” trained their logistic regression model for 55 seconds only, which is six times faster than our model. Our Logistic Regression model is fastest in blind test evaluation time, tying with Section 5 Group 1. \n",
    "\n",
    "Hence, our logistic regression model is one of the best performers in terms of accuracy and speed, but there is room for improvement, which we can do in future work. This will include continuation of experimentations under SMOTE oversampling or other strategies. We will consider more hyperparameter tuning and more sophisticated models. The threats that we foresee are the significant amount of time to train and debug, as well as communication issues with other teams  for more accurate comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ac618d2-01da-4e4b-8557-5b83540cf884",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 7. Conclusions and Next Steps\n",
    "\n",
    "U.S flights departure delays result in financial losses and customer dissatisfaction. Hence, the major focus of this project is to accurately report to the airline leadership team, a machine learning based prediction using airline and weather features of flight delays so as to plan alternatives for customers thereby increasing customer satisfaction. Our hypothesis is experimentally true about engineered features improving prediction modeling performance, so we will utilize this strategy. We considered decision tree for prediction task due to its short training and inference times while performing among the best. We considered logistic regression for classification task due to its short training and inference times while performing among the best. Our models are competitive, performing among the best when compared to other project teams. The result of this research would help airlines to give more accurate flight predictions under hectic schedule resulting in better customer experience.\n",
    "\n",
    "For future work, we will expand feature engineering to consider the arrival airport, create graph features like the page-rank of the origin/desintation airport, expand the scope of hyper-parameter tuning and consider further down-sampling for gradient boosted decision tree training. We will also include more details around each model iteration such as the feature importance, and plots of precision/recall curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c633d6c-5cb9-42ed-8218-7fc4c04707c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 8. Citations\n",
    "\n",
    "1. General. BTS. (n.d.). Retrieved November 30, 2022, from https://www.transtats.bts.gov/OT_Delay/OT_DelayCause1.asp"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "da28decb-fd4c-4918-89e6-b95334940584",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "fe197adc-e1c3-436b-999b-0ef083dfcacd",
     "origId": 1714182546275170,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Section7-Group2-Team24-Phase4_MAIN (1)",
   "notebookOrigID": 1714182546275159,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
